version: "3.0"

volumes:
  # Named volume containing kubeconfig, shared by different containers 
  kube:


services:

  # Local Docker registry    
  local-registry:
    image: "registry:2.7.1"
    ports:
      - "5000:5000"

  # Kubernetes control plane
  server:
    image: "rancher/k3s:v1.22.7-k3s1"
    command: server --disable traefik
    privileged: true
    environment:
      - K3S_TOKEN=k3stoken
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - "./k3s/registries.yaml:/etc/rancher/k3s/registries.yaml"
      - "kube:/output"
    ports:
      - "6443:6443"
      - "80:80"

  # Helper container for copying kubeconfig to host directory
  kubeconfig:
    image: alpine
    restart: on-failure
    volumes:
      - "kube:/.kube"
      - "${PWD}:/tmp"
    entrypoint: sh -ec
    command:
      - |
        cp /.kube/kubeconfig.yaml /tmp/kubeconfig
        chmod 666 /tmp/kubeconfig

  spark-operator-init:
    image: alpine/helm:3.6.3
    restart: on-failure
    volumes:
      - "kube:/.kube"
    entrypoint: sh -ec
    command:
      - |
        [ -f "/.kube/kubeconfig.yaml" ] || exit 5
        mkdir -p /.kube
        cp /.kube/kubeconfig.yaml /.kube/config
        sed -i 's/127.0.0.1/server/' /.kube/config
        helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
        helm install --insecure-skip-tls-verify spark spark-operator/spark-operator --version 1.1.6 --kubeconfig /.kube/config --wait

  # Kubernetes worker nodes
  agent:
    image: "rancher/k3s:v1.22.7-k3s1"
    deploy:
      replicas: 3
    privileged: true
    environment:
      - K3S_URL=https://server:6443
      - K3S_TOKEN=k3stoken
    volumes:
      - "./k3s/registries.yaml:/etc/rancher/k3s/registries.yaml"

  # Install Istio on Kubernetes 
  istio:
    image: bitnami/kubectl:1.22.7
    restart: on-failure
    user: root:root
    volumes:
      - "kube:/.kube"
    entrypoint: bash -ec
    command:
      - |
        cp /.kube/kubeconfig.yaml /.kube/config
        sed -i 's/127.0.0.1/server/' /.kube/config
        kubectl cluster-info
        curl -sL https://istio.io/downloadIstioctl | ISTIO_VERSION=1.13.3 sh -
        /.istioctl/bin/istioctl install -y

  # Install Knative serving on Kubernetes
  knative:
    image: bitnami/kubectl:1.22.7
    restart: on-failure
    user: root:root
    volumes:
      - "kube:/.kube"
      - "../../cluster-init/knative-configmaps/config-features.yaml:/config-features.yaml"
    entrypoint: bash -ec
    command:
      - |
        cp /.kube/kubeconfig.yaml /.kube/config
        sed -i 's/127.0.0.1/server/' /.kube/config
        kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.3.2/serving-crds.yaml
        kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.3.2/serving-core.yaml
        kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.3.2/serving-hpa.yaml
        kubectl apply -f /config-features.yaml
        kubectl -n knative-serving patch configmap/config-deployment --type merge -p '{"data":{"registriesSkippingTagResolving": "localhost:5000"}}'
        kubectl -n knative-serving patch configmap/config-domain --type merge -p '{"data":{"127.0.0.1.nip.io":""}}'
        kubectl apply -f https://github.com/knative-sandbox/net-istio/releases/download/knative-v1.3.0/net-istio.yaml

  mlp-postgres:
    image: bitnami/postgresql:13.0.0
    environment:
      POSTGRESQL_DATABASE: mlp
      POSTGRESQL_USERNAME: mlp
      POSTGRESQL_PASSWORD: mlp

  mlp-postgres-init:
    image: migrate/migrate:v4.13.0
    restart: on-failure
    volumes:
      - "./mlp/db-migrations:/db-migrations"
    entrypoint: sh -ec
    command:
      - migrate -path=/db-migrations -database=postgres://mlp:mlp@mlp-postgres:5432/mlp?sslmode=disable up

  mlp:
    image: ghcr.io/gojek/mlp:v1.1.0-alpha
    restart: on-failure
    env_file: ./mlp/dev.env
    ports:
      - "8081:8080"

  mlp-init:
    image: curlimages/curl:7.73.0
    restart: on-failure
    entrypoint: sh -ec
    command:
      - |
        curl \
          -X POST 'http://mlp:8080/v1/projects' \
          --header 'Content-Type: application/json' \
          --data-raw '{"name": "default","team": "default","stream": "default"}'
        curl 'http://mlp:8080/v1/projects' | grep '"id":1' 

  vault:
    image: vault:1.6.0
    command: server -dev -dev-kv-v1 -dev-root-token-id=root
    ports:
      - "8200:8200"

  vault-init:
    image: vault:1.6.0
    restart: on-failure
    volumes:
      - "kube:/.kube"
    environment:
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: root
    entrypoint: sh -ec
    command:
      - |
        cat /.kube/kubeconfig.yaml | grep -q 'server: https://127.0.0.1:6443'
        wget -O yq https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64
        install yq /usr/bin/yq
        cat <<EOF > /tmp/secret-k3s.json
        {
          "master_ip": "$$(cat /.kube/kubeconfig.yaml | yq read - 'clusters[0].cluster.server')",
          "certs": "$$(cat /.kube/kubeconfig.yaml | yq read - 'clusters[0].cluster.certificate-authority-data' | base64 -d | awk 1 ORS='\\n')",
          "client_certificate": "$$(cat /.kube/kubeconfig.yaml | yq read - 'users[0].user.client-certificate-data' | base64 -d | awk 1 ORS='\\n')",
          "client_key": "$$(cat /.kube/kubeconfig.yaml | yq read - 'users[0].user.client-key-data' | base64 -d | awk 1 ORS='\\n')"
        }
        EOF
        vault kv put secret/k3s @/tmp/secret-k3s.json

  merlin-postgres:
    image: bitnami/postgresql:13.0.0
    environment:
      POSTGRESQL_DATABASE: merlin
      POSTGRESQL_USERNAME: merlin
      POSTGRESQL_PASSWORD: merlin

  merlin:
    image: ghcr.io/gojek/merlin:v0.8.0-alpha
    restart: on-failure
    volumes:
      - "./merlin/deployment-config.yaml:/deployment-config.yaml"
    env_file: ./merlin/dev.env
    ports:
      - "8082:8080"

  turing-postgres:
    image: bitnami/postgresql:13.0.0
    environment:
      POSTGRESQL_DATABASE: turing
      POSTGRESQL_USERNAME: turing
      POSTGRESQL_PASSWORD: turing
    ports:
      - "5432:5432"

  mockserver:
    image: bitnami/kubectl:1.22.7
    restart: on-failure
    user: root:root
    volumes:
      - "../../e2e/turing.mockserver.yaml:/mockserver.yaml"
      - "kube:/.kube"
    entrypoint: bash -ec
    command:
      - |
        cp /.kube/kubeconfig.yaml /.kube/config
        sed -i 's/127.0.0.1/server/' /.kube/config
        kubectl apply -f /mockserver.yaml
